# Word2Vec from First Principles

A complete, mathematical derivation and implementation of Word2Vec with **zero black-box abstractions**.

> **"If you can't implement it from scratch, you don't understand it."**

This project treats Word2Vec as what it actually is: a **parametric conditional language model optimized via maximum likelihood estimation**, not neural magic.

---

## ğŸ¯ Project Goals

- **No high-level libraries** (no gensim, no TensorFlow, no PyTorch)
- **Every equation maps to code** - direct, traceable, explicit
- **Complete mathematical derivation** - from probability theory to gradient descent
- **Manual gradient computation** - no autograd, no backprop frameworks
- **Statistical rigor** - maximum likelihood, not hand-waving

## What This Is

Word2Vec is:
- A **statistical model** of word co-occurrence patterns
- A **low-rank approximation** of a shifted PMI (Pointwise Mutual Information) matrix  
- A **maximum likelihood estimator** for conditional probabilities: P(context | center)
- A **logistic regression** model with negative sampling

Word2Vec is **not**:
- A deep neural network
- "AI learning meaning."
- Magic embeddings

---

## Mathematical Foundation

### The Core Objective

Given corpus tokens wâ‚, wâ‚‚, ..., wâ‚œ, we maximize:

```
L(Î¸) = âˆ P(wâ‚œâ‚Šâ±¼ | wâ‚œ; Î¸)
```

Or equivalently, minimize negative log-likelihood:

```
J(Î¸) = -1/|T| âˆ‘â‚œ âˆ‘â±¼ log P(wâ‚œâ‚Šâ±¼ | wâ‚œ; Î¸)
```

### The Model

For each word w, we learn two d-dimensional vectors:
- **vw**: input (center) embedding
- **uw**: output (context) embedding

The conditional probability using softmax:

```
P(wâ‚’ | wc) = exp(uâ‚’áµ€vc) / âˆ‘w exp(uwáµ€vc)
```

### The Problem

Softmax normalization requires O(|V|) operations per training pair.

**Solution**: Negative Sampling

### Negative Sampling Objective

Replace multi-class softmax with binary logistic classification:

```
J_NS = -log Ïƒ(uâ‚’áµ€vc) - âˆ‘áµ¢â‚Œâ‚áµ ğ”¼[log Ïƒ(-uáµ¢áµ€vc)]
```

where:
- Ïƒ(x) = 1/(1 + exp(-x)) is the sigmoid function
- k negative samples drawn from noise distribution Pâ‚™(w) âˆ count(w)^0.75

### Gradients (Computed Manually)

```
âˆ‚J/âˆ‚uâ‚’ = [Ïƒ(uâ‚’áµ€vc) - 1] vc
âˆ‚J/âˆ‚uáµ¢ = Ïƒ(uáµ¢áµ€vc) vc
âˆ‚J/âˆ‚vc = [Ïƒ(uâ‚’áµ€vc) - 1] uâ‚’ + âˆ‘áµ¢ Ïƒ(uáµ¢áµ€vc) uáµ¢
```

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/yourusername/word2vec-from-scratch.git
cd word2vec-from-scratch
pip install numpy  # That's it. No other dependencies.
```

### Basic Usage

```python
from word2vec import Word2Vec

# Your corpus
corpus = """
The king sat on the throne.
The queen stood beside the king.
Paris is the capital of France.
"""

# Initialize model
model = Word2Vec(
    embedding_dim=100,    # dimension d
    window_size=5,        # context window Â±5
    neg_samples=5,        # k negative samples
    learning_rate=0.025,  # initial Î±
    min_count=5          # minimum word frequency
)

# Train
model.train(corpus, epochs=10)

# Get embeddings
king_vec = model.get_embedding('king')

# Find similar words
similar = model.most_similar('king', topn=5)
# [('queen', 0.876), ('prince', 0.834), ...]

# Solve analogies: king - man + woman â‰ˆ ?
result = model.analogy('king', 'man', 'woman')
# [('queen', 0.891), ...]
```

---

## ğŸ“Š Implementation Details

### Architecture

```
Corpus â†’ Tokenization â†’ Vocabulary Building
    â†“
Training Pairs: (center, context)
    â†“
Parameters: V âˆˆ â„^(|V| Ã— d), U âˆˆ â„^(|V| Ã— d)
    â†“
For each pair (wc, wâ‚’):
    1. Sample k negative words from Pâ‚™
    2. Compute loss: J_NS
    3. Compute gradients (manually)
    4. SGD update: Î¸ â† Î¸ - Î±âˆ‡J
```

### Parameter Count

Total trainable parameters: **2 Ã— |V| Ã— d**

Example: 10,000 vocabulary Ã— 100 dimensions = **2,000,000 parameters**

Per training step updates:
- 1 input embedding (vc)
- 1 positive output embedding (uâ‚’)  
- k negative output embeddings (uáµ¢)

**Only (k+2) vectors updated per step** â†’ sparse, efficient updates

### Noise Distribution

```python
Pâ‚™(w) = count(w)^0.75 / âˆ‘w' count(w')^0.75
```

Why 0.75?
- Unigram (Î±=1.0): over-samples common words like "the"
- Uniform (Î±=0.0): over-samples rare words
- Smoothed (Î±=0.75): empirically optimal balance

---

## ğŸ”¬ Why This Works

### Connection to PMI

Word2Vec implicitly factorizes a shifted PMI matrix:

```
uâ‚’áµ€vc â‰ˆ PMI(wâ‚’, wc) - log k
```

where PMI (Pointwise Mutual Information):

```
PMI(wâ‚’, wc) = log[P(wâ‚’, wc) / (P(wâ‚’)P(wc))]
```

### Why Cosine Similarity Works

Embeddings encode **distributional statistics**:
- Words with similar contexts â†’ similar vectors
- Direction encodes semantic category
- Magnitude encodes frequency (normalized away by cosine)

### Why Analogies Emerge

Vector arithmetic works when PMI differences are approximately parallel:

```
PMI(queen, c) - PMI(king, c) â‰ˆ PMI(woman, c) - PMI(man, c)
```

This translates to:

```
vqueen - vking â‰ˆ vwoman - vman
```

**This is approximate, not exact** - depends on dimensionality and training.

---

## ğŸ“ Project Structure

```
word2vec-from-scratch/
â”œâ”€â”€ word2vec.py          # Complete implementation
â”œâ”€â”€ README.md            # This file
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py   # Simple examples
â”‚   â”œâ”€â”€ analogy_test.py  # Analogy evaluation
â”‚   â””â”€â”€ similarity.py    # Similarity benchmarks
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ theory.md        # Full mathematical derivation
â”‚   â”œâ”€â”€ gradients.md     # Gradient derivations
â”‚   â””â”€â”€ implementation.md # Code walkthrough
â””â”€â”€ tests/
    â””â”€â”€ test_word2vec.py # Unit tests
```

---

## ğŸ“ Educational Components

### 1. Complete Mathematical Derivation

Every step from probability theory to working code:
- Corpus definition and notation
- Softmax formulation
- Gradient derivation (chain rule, step-by-step)
- Computational complexity analysis
- Negative sampling derivation
- PMI connection proof

### 2. Explicit Implementation

No hidden abstractions:
```python
def _update_embeddings(self, center_idx, context_idx):
    """Manual SGD update with explicit gradients."""
    v_c = self.V[center_idx]
    u_o = self.U[context_idx]
    
    # Forward pass
    score_pos = np.dot(u_o, v_c)
    sigmoid_pos = self._sigmoid(score_pos)
    
    # Gradient computation (manual)
    grad_u_o = (sigmoid_pos - 1.0) * v_c
    grad_v_c = (sigmoid_pos - 1.0) * u_o
    
    # SGD update (manual)
    self.U[context_idx] -= self.alpha * grad_u_o
    # ... negative samples ...
    self.V[center_idx] -= self.alpha * grad_v_c
```

### 3. Statistical Interpretation

Clear connections to:
- Maximum likelihood estimation
- Exponential family models
- Matrix factorization
- Information theory (PMI, entropy)

---

## âš¡ Performance Notes

### Computational Complexity

- **Per training pair**: O(k Ã— d) where k â‰ª |V|
- **Full softmax**: O(|V| Ã— d) â†’ **infeasible for large vocabularies**
- **Speedup**: ~20,000Ã— for |V|=100K, k=5

### Training Time

On modest hardware (CPU):
- 1M training pairs, 100-dim, 5 epochs: ~2-5 minutes
- 100M pairs: ~3-5 hours

For production use on large corpora, consider:
- Subsampling frequent words
- Phrase detection
- Parallel processing (pure NumPy is easily parallelizable)

---

## ğŸ§ª Validation

### Intrinsic Evaluation

1. **Similarity**: Cosine similarity correlates with human judgments
2. **Analogies**: Semantic and syntactic analogy accuracy
3. **Clustering**: K-means on embeddings recovers semantic categories

### Extrinsic Evaluation

Use learned embeddings for:
- Sentiment analysis
- Named entity recognition
- Document classification
- Machine translation

**Note**: This implementation prioritizes clarity over performance. For production, see limitations below.

---

## ğŸ“ Limitations & Extensions

### Current Limitations

- **Static embeddings**: One vector per word type (no polysemy handling)
- **No subword information**: Can't handle OOV words
- **Context-independent**: Word order beyond window ignored
- **Single-threaded**: No parallelization

### Possible Extensions

1. **Subword embeddings** (FastText):
   ```
   vword = âˆ‘ vn-gram / |n-grams|
   ```

2. **Dynamic context window**:
   Already implemented - randomly samples window size

3. **Phrase detection**:
   Identify multi-word expressions (e.g., "New York")

4. **Hierarchical softmax**:
   Alternative to negative sampling using binary tree

5. **Contextual embeddings**:
   Use as initialization for LSTM/Transformer models

---

## ğŸ“š References

### Original Papers

- **Mikolov et al. (2013)**: "Efficient Estimation of Word Representations in Vector Space"
- **Mikolov et al. (2013)**: "Distributed Representations of Words and Phrases and their Compositionality"

### Theoretical Analysis

- **Levy & Goldberg (2014)**: "Neural Word Embedding as Implicit Matrix Factorization"
- **Goldberg & Levy (2014)**: "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method"

### Related Work

- **GloVe** (Pennington et al., 2014): Explicit co-occurrence matrix factorization
- **FastText** (Bojanowski et al., 2017): Subword embeddings
- **ELMo, BERT**: Contextual embeddings (beyond this project's scope)

---

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:

- **Performance optimizations** (while maintaining clarity)
- **Visualization tools** (t-SNE, PCA of embeddings)
- **Evaluation benchmarks** (word similarity, analogies)
- **Documentation improvements**
- **Additional examples**

Please maintain the principle: **every line of code must map to a mathematical operation**.

---

## ğŸ“œ License

MIT License - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

This implementation is inspired by:
- The clarity of Mikolov et al.'s original papers
- Goldberg & Levy's theoretical analysis
- The principle that **understanding requires implementation**

Special thanks to the NLP community for decades of research on distributional semantics.

---

## ğŸ’¬ FAQ

**Q: Why implement from scratch when libraries exist?**  
A: Understanding. You can't claim to understand Word2Vec if you can't derive and implement it.

**Q: Is this production-ready?**  
A: No. This prioritizes educational clarity. For production, use gensim or pre-trained embeddings.

**Q: Why NumPy only?**  
A: To remove all abstractions. Every operation is explicit and traceable.

**Q: What about BERT/GPT/transformers?**  
A: Those are different models. Word2Vec is a foundational technique worth understanding deeply.

**Q: Can I use this for my research?**  
A: Yes (MIT license), but cite appropriately and consider using established libraries for benchmarking.

---

## ğŸ“§ Contact

Questions? Suggestions? Open an issue or reach out!

**Remember**: If you understand Word2Vec deeply, you understand:
- Maximum likelihood estimation
- Matrix factorization  
- Distributional semantics
- The foundations of modern NLP

Happy learning! ğŸš€

---

*Built with â¤ï¸ and âˆ‡ (gradients)*
